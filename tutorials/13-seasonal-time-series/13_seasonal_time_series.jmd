---
title: Bayesian Time Series Analysis
permalink: /:collection/:name/
weave_options:
  error : false
---

In time series analysis we are often interested in understanding how various real-life circumstances impact our quantity of interest.
These can be, for instance, season, day of week, or time of day.
To analyse this it is useful to decompose time series into simpler components (corresponding to relevant circumstances)
and infer their relevance.
In this tutorial we are going to use Turing for time series analysis and learn about useful ways to decompose time series.

# Modelling time series
Let us briefly discuss some time series decomposition techniques before we start coding.
It is useful to take a divide-and-conquer approach and decompose time series into either *additive* or *multiplicative* components.
For instance, the time series $f(t)$ can be decomposed into a sum of $n$ components

$$f(t) = f_1(t) + f_2(t) \dots + f_n(t),$$

or we can decompose $g(t)$ into a product of $m$ components

$$g(t) = g_1(t) \times g_2(t) \dots \times g_m(t).$$

An additive decomposition implies that the terms to not interact, and a multiplicative decomposition implies that the magnitude of one component will affect the others.
This type of decomposition is great since it lets us reason about individual components, which helps us interpret their meaning.
Two common components are *trends*, which represent the overall change of the time series (often assumed to be linear),
and *cyclic effects* which contribute oscillating effects around the trend.
Let us simulate some data with both a linear trend and oscillating effects in an additive structure.

```julia
using Plots, StatsPlots, Turing, Statistics
true_sin_freq = 2
true_cos_freq = 7
tmax = 10
tt = 0:0.05:tmax
f₁(t) = -1 + 2*t
f₂(t) = 5*sin(2π*t*true_sin_freq/tmax)
f₃(t) = 2.5*cos(2π*t*true_cos_freq/tmax)
f(t) = f₁(t) + f₂(t) + f₃(t)
lw = 1

plt_target = plot(
    f, tt,
    label="f(t)",
    title="Observed time series",
    legend=:topleft,
    color=1,
    linewidth=3
)
plot!(plt_target, f₁, tt, label="f₁(t)", color=2, style=:dot, linewidth=lw)
plot!(plt_target, f₂, tt, label="f₂(t)", color=3, style=:dash, linewidth=lw)
plot!(plt_target, f₃, tt, label="f₃(t)", color=4, style=:dashdot, linewidth=lw)
```

We see that simple components, when combined, can give rise to fairly complex time series.

# Model fitting
Having discussed time series decomposition, let us fit a model to the time series above and recover the true parameters.
Before building our model, we standardise the time axis to $[0, 1]$ and subtract the max of the time series.
This helps convergence while maintaining interpretability and the correct scales for the cyclic components.

```julia
σ = 0.35
t = collect(tt)[begin:3:end]
t_min, t_max = extrema(t)
x = (t .- t_min) ./ (t_max - t_min)
y = f.(t) .+ σ*randn(size(t))
y_max = maximum(y)
y = y .- y_max

scatter(x, y, title="Standardised data",label=nothing)

```
Let us now build our model.
We want to assume a linear trend, and cyclic effects.
Encoding a linear trend is easy enough, but what about cyclical effects?
We will take a scattergun approach, and create multiple cyclical features using both sine and cosine functions and let our inference machinery figure out which to keep.
To do this, we define how long a one period should be, and create features in reference to said period.
How long a period should be is problem dependent, but as an example let us say it is $1$ year.
If we then find evidence for a cyclic effect with a frequency of 2, that would mean a biannual effect. A frequency of 4 would mean quarterly etc.
Since we are using synthetic data, we are simply going to let the period be 1, which is the entire length of the time series.

```julia
freqs = 1:10
num_freqs = length(freqs)
period = 1
cyclic_feature(x, freqs, p, fn) = mapreduce(f -> fn.(2*π*f*x/p), hcat, freqs)
c = hcat(cyclic_feature(x, freqs, period, sin), cyclic_feature(x, freqs, period, cos))

plot_freqs = [1, 3, 5]
freq_ptl = plot(
    c[:, plot_freqs],
    label=reshape(["sin(2π$(f)x)" for f in plot_freqs], 1, :),
    title="Cyclical features subset"
)
```

Having constructed the cyclical features, we can finally build our model. The model we will implement looks like this

$$
f(t) = \alpha + \beta_t t + \sum_{i=1}^F \beta_{\text{sin},i} \text{sin}(2\pi f_i t) + \sum_{i=1}^F \beta_{\text{cos},i} \text{cos}(2\pi f_i t),
$$

with a Gaussian likelihood $y \sim \mathcal{N}(f(t), \sigma)$.
For convenience we are treating the cyclical feature weights $\beta_{\text{sin},i} and \beta_{\text{cos},i}$ the same in code and weight them with $\beta_c$.
And just because it is so easy, we parameterise our model with the operation with which to apply the cyclic effects.
This lets us use the exact same code for both additive and multiplicative models.
Finally, we plot prior predictive samples to make sure our priors make sense.

```julia
@model function model(t, c, y, op)
    α ~ Normal(0, 10)
    βt ~ Normal(0, 2)
    βc ~ MvNormal(zeros(size(c, 2)), 1)
    σ ~ TruncatedNormal(0, .1, 0, Inf)

    cyclic = c * βc
    trend = α .+ βt .* t
    μ = op(trend, cyclic)
    y ~ MvNormal(μ, σ)
    (; trend, cyclic)
end

prior_samples = sample(model(t, c, missing, +), Prior(), 100)
y_prior_samples = group(prior_samples, :y).value.data[:,:,1]
prior_plt = plot(title="Prior samples")
plot!(prior_plt, t, y_prior_samples', label=nothing, color=1, linewidth=1, alpha=0.5)
scatter!(prior_plt, t, y, label="Data", color=2)
```

With the model specified and with a reasonable prior we can now let Turing decompose the time series for us!

```julia
function mean_ribbon(samples)
    qs = quantile(samples)
    low = qs[:,Symbol("2.5%")]
    up =  qs[:,Symbol("97.5%")]
    m = mean(samples)[:,:mean]
    m, (m - low, up - m)
end

function get_decomposition(x, c, chain)
    chain_params = Turing.MCMCChains.get_sections(chain, :parameters)
    generated_quantities(model(x, c, missing, +), chain_params)
end

function plot_fit(x, y, decomp, ymax)
    trend = mapreduce(x -> x.trend, hcat, decomp)
    cyclic = mapreduce(x -> x.cyclic, hcat, decomp)

    trend_plt = plot(
        x, trend .+ ymax,
        color=1,
        label=nothing,
        alpha=0.2,
        title="Trend",
        xlabel="Time",
        ylabel="f₁(t)"
    )
    ls = [ones(length(t)) t] \ y
    α̂, β̂ = ls[1], ls[2:end]
    plot!(
        trend_plt,
        t, α̂ .+ t.*β̂ .+ ymax,
        label="Least squares trend",
        color=5,
        linewidth=4
    )

    scatter!(
        trend_plt,
        x, y .+ ymax,
        label=nothing,
        color=2,
        legend=:topleft
    )
    cyclic_plt = plot(
        x,
        cyclic,
        color=1,
        label=nothing,
        alpha=0.2,
        title="Cyclic effect",
        xlabel="Time",
        ylabel="f₂(t)",
    )
    trend_plt, cyclic_plt
end

chain = sample(model(x, c, y, +), NUTS(), 2000)
y_samples = predict(model(x, c, missing, +), chain)
m, conf = mean_ribbon(y_samples)
predictive_plt = plot(
    t, m .+ y_max,
    ribbon=conf,
    label="Posterior density",
    title="Posterior decomposition",
    xlabel="Time",
    ylabel="f(t)"
)
scatter!(predictive_plt, t, y .+ y_max, color=2, label="Data", legend=:topleft)

decomp = get_decomposition(x, c, chain)
decomposed_plt = plot_fit(t, y, decomp, y_max)
combined_plt = plot(
    predictive_plt,
    decomposed_plt...,
    layout=(4, 1),
    size=(700, 1000)
)
```

Inference is successful and the posterior beautifully captures the data.
We see that the least squares linear fit deviates somewhat from the posterior trend.
Since our model takes cyclic effects into account separately,
we get a better estimate of the true overall trend than if we would have just fitted a line.
But what frequency content did the model identify?

```julia
βc = group(chain, :βc).value
βsin = βc[:,begin:num_freqs,:]
βcos = βc[:,num_freqs+1:end,:]
labels = reshape(["freq = $i" for i in freqs], 1,:)
colors = collect(freqs)'
style = reshape([i <= 10 ? :solid : :dash for i in 1:length(labels)], 1,:)
sin_features_plt = density(
    βsin[:,:,1],
    title="Sine features posterior",
    label=labels,
    ylabel="Density",
    xlabel="Weight",
    color=colors,
    linestyle=style,
    legend=:bottom
)
cos_features_plt = density(
    βcos[:,:,1],
    title="Cosine features posterior",
    ylabel="Density",
    xlabel="Weight",
    label=nothing,
    color=colors,
    linestyle=style
)

seasonal_features_plt= plot(
    sin_features_plt,
    cos_features_plt,
    layout=(2,1),
    size=(800, 600)
)
```

Plotting the posterior over the cyclic features reveals that the model managed to extract the true frequency content.
