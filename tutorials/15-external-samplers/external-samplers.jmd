---
title: Using External Sampler
permalink: /tutorials/:name/
mathjax: true
weave_options:
  error : false
---

# Using External Samplers

Turing currently wraps some of the most popular sampling libraries in the Julia ecosystem.
These wrappers allow new users to seemlessly sample statistical models without leaving Turing.
However, these wrappers often lack the latests functionalities implemented on the sampling libraries that more experienced users might want to use.
Moreover, users might want to use samplers currently not wrapped within Turing.

For these reasons, Turing now allows users the possibility of defining their own sampler and using it to sample a Turing model.
Let us begin by defining a simple Turing model to showcase the use of external samplers.
For this demonstration we will use a 10 dimensional Neal's funnel:

```julia
# Import libraries.
using Turing, Random, LinearAlgebra

d = 10
@model function funnel()
    θ ~ Truncated(Normal(0, 3), -3, 3)
    z ~ MvNormal(zeros(d - 1), exp(θ) * I)
    return x ~ MvNormal(z, I)
end
```

Let us know sample the model to generate some observations and condition our model on them:

```julia
Random.seed!(1)
(; x) = rand(funnel() | (θ=0,))
model = funnel() | (; x)
```

Users can now use any sampler algorithm they wish to sample this model as long as it is of the type `AbstractMCMC.AbstractSampler`.
Before discussing how is this in done in practice, it is interesting to give a high level description of the process.
Imagine that we created an instance of external sampler that we will call `spl` such that `typeof(spl)<:AbstractMCMC.AbstractSampler`.
In order to avoid type ambiguity within Turing, at the moment is necessary to declare `spl` as an external sampler to Turing this is done as:

```julia
espl = externalsampler(spl)
```

where `externalsampler(s::AbstractMCMC.AbstractSampler)` is a Turing function that types our external sampler adquately.

Arrived at this point sampling our model is as easy as:

```julia
chain = sample(model, espl, 1_000)
```

This process will use `AbstractMCMC` functions overloaded by Turing to generate chains identical to those generated by the native samplers.
Note that if the external sampler produces transitions that Turing cannot parse the bundling of the samples will be different or fail.

## AdvancedMH

A good point to start to show how this is done in practice is by looking at the sampling library `AdvancedMH` for Metropolis Hastings methods.
Let's say that we want to use a random walk Metropolis Hastings sampler without specifying the proposal distributions.
We can do this in two different ways:

```julia
# Importing the sampling library
using AdvancedMH

rwmh_1 = RWMH(d)
rwmh_2 = RWMH(model)
```

The first method uses a multivariate Gaussian distribution with zero mean and unit variance in `d` dimensions.
The second method uses the priors of the model as a proposal. Note that the second method requires Turing to be imported.

Let us now compare the performance and posteriors of these two samplers to the native MH sampler within Turing.

```julia
mh_chain_0 = sample(model, MH(), 1_000)
mh_chain_1 = sample(model, externalsampler(rwmh_1), 1_000)
mh_chain_2 = sample(model, externalsampler(rwmh_2), 1_000)
```

## AdvancedHMC

As previously mentioned the Turing wrappers can often limit the capabilities of the sampling libraries they wrap.
AdvancedHMC is a clear example of this. A common practice when performing HMC is to provide an initial guess for the mass matrix.
However, the native HMC sampler within Turing only allows the user to specify the type of the mass matrix despite the two options being possible within AdvancedHMC.
Thanfully, we can use Turing's support for external samplers to define a HMC sampler with a custom mass matrix in AdvancedHMC and then use to sample our Turing model.

## Micro-Canonical HMC

So far we have used to Turing's support for external samplers to go beyond the capabilities of the wrappers.
Now we want to use this support to employ a sampler not supported within Turing's ecosystem yet.
We will use the recently developped Micro-Cannoncial Hamiltonian Monte Carlo (MCHMC) sampler to showcase this.
MCHMC[^1,^2] is HMC sampler that conserves energy along the exploration of the phase space.
This is achieved by simulating the dynamics of a microcanonical Hamiltonian with an additional noise term to ensure ergodicity.
The algorithm has been shown to outperform traditioanl HMC in many scenarios and it doesn't require of Metropolis adjustments.

# Refences

[^1]: Robnik et al, (Microcanonical Hamiltonian Monte Carlo)[https://arxiv.org/abs/2212.08549], 2022
[^2]: Robnik and Seljak, (Langevine Hamiltonian Monte Carlo)[https://arxiv.org/abs/2303.18221], 2023
