---
title: Getting Started
permalink: /docs/using-turing/get-started
redirect_from: docs/0-gettingstarted/
weave_options:
  error : false
---

# Getting Started

## Installation

To use Turing, you need to install Julia first and then install Turing.

### Install Julia

You will need to install Julia 1.3 or greater, which you can get from [the official Julia website](http://julialang.org/downloads/).

### Install Turing.jl

Turing is an officially registered Julia package, so you can install a stable version of Turing by running the following in the Julia REPL:

```julia
using Pkg
Pkg.add("Turing")
```

You can check if all tests pass by running `Pkg.test("Turing")` (it might take a long time)

## Example

Here's a simple example showing Turing in action.

First, we can load the Turing and StatsPlots modules

```julia
using Turing
using StatsPlots
```

Then, we define a simple Normal model with unknown mean and variance

```julia
@model function gdemo(x, y)
    s² ~ InverseGamma(2, 3)
    m ~ Normal(0, sqrt(s²))
    x ~ Normal(m, sqrt(s²))
    return y ~ Normal(m, sqrt(s²))
end
```

Then we can run a sampler to collect results. In this case, it is a Hamiltonian Monte Carlo sampler

```julia
chn = sample(gdemo(1.5, 2), HMC(0.1, 5), 1000)
```

We can plot the results

```julia
plot(chn)
```

In this case, because we use the [normal-inverse gamma distribution](https://en.wikipedia.org/wiki/Normal-inverse-gamma_distribution)
as a [conjugate prior](https://en.wikipedia.org/wiki/Conjugate_prior), we can compute
its updated mean as follows:

```julia
s² = InverseGamma(2, 3)
m = Normal(0, 1)
data = [1.5, 2]
x_bar = mean(data)
N = length(data)

mean_expectation = (m.σ * m.μ + N * x_bar) / (m.σ + N)
```

We can also compute the updated variance

```julia
updated_alpha = shape(s²) + (N / 2)
updated_beta =
    scale(s²) +
    (1 / 2) * sum((data[n] - x_bar)^2 for n in 1:N) +
    (N * m.σ) / (N + m.σ) * ((x_bar)^2) / 2
variance_expectation = updated_beta / (updated_alpha - 1)
```

Finally, we can check if these expectations align with our approximations

```julia
# Plot the posterior distribtion, calculated analytically
updated_distribution = Normal(mean_expectation, sqrt(variance_expectation))
samples = rand(updated_distribution, 1000)
density(samples, label="Posterior (Analytical)", w=3)

# Compare with the approximate posterior distribtion from HMC
approx_mean = mean(chn[:m]) 
approx_var = mean(chn[:s²]) 
approximated_distribution = Normal(approx_mean, sqrt(approx_var))
density!(rand(updated_distribution, 1000), label="Posterior (HMC)", w=3)
```
