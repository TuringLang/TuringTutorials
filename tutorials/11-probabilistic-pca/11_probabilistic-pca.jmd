---
title: Probabilistic Principal Component Analysis
permalink: /:collection/:name/
weave_options:
  error : false
---

Principal component analysis is a fundamental technique to analyse and visualise data.
You may have come across it in many forms and names.
Here, we give a probabilistic perspective on PCA with some biologically motivated examples.
For more details and a mathematical derivation, we recommend Bishop's textbook "Pattern Recognition and Machine Learning".

The idea of PCA is to find a latent variable $z$ that can be used to describe the hidden structure in our dataset.
In the probabilistic PCA model we use a simple Gaussian prior
$$
p(z) = \mathcal{N}(z; 0, I)
$$
for $z$, and similarly the conditional distribution
$$
p(x \,|\, z) = \mathcal{N}(x; W z + \mu, \sigma^2 I)
$$
is modeled via a Gaussian distribution.

### A biologically motivated example

We'll generate synthetic data to explore the models.
The simulation is inspired by biological measurement of expression of genes in cells, and so you can think of the two variables as cells and genes.
While the human genome is (mostly) identical between all the cells in your body, there exist interesting differences in gene expression in different human tissues and disease conditions.
Similarly, one way to investigate certain diseases is to look at differences in gene expression in cells from patients and healthy controls (usually from the same tissue).

Usually, we can assume that the changes in gene expression only affect a subset of all genes (and these can be linked to diseases in some way).
One of the challenges of this kind of data is to see the underlying structure, e.g. to make the connection between a certain state (healthy/disease) and gene expression.
The problem is that the space we are investigating is very large (Up to 20000 genes across 1000s of cells). So in order to find structure in this data, we usually need to project the data into a lower dimensional space.

If you are not that interested in the biology, the more abstract problem formulation is to project a high-dimensional space onto a different space - in the case of PCA - a space where most of the variation is concentrated in the first few dimensions.
So you will use PCA to explore underlying structure in your data that is not necessarily obvious from looking at the raw data itself.

First we load the required packages.

```julia
using Turing
using FillArrays
using LinearAlgebra

# Visualization
using StatsPlots

# Import Fisher's iris example data set
using RDatasets

# Set a seed for reproducibility.
using Random
Random.seed!(1789);
```

Here, we simulate the biological problem described earlier and create an expression matrix with some non-informative genes.
Admittedly, this is a very simplistic example with far fewer cells and genes and a very straighforward relationship.
We try to capture the essence of the problem, but then, unfortunately, real life problems tend to be much more messy.

```julia
ngenes = 9
ncells = 60
expression_matrix = randn(ngenes, ncells)
expression_matrix[1:(ngenes ÷ 3), 1:(ncells ÷ 2)] .+= 10
expression_matrix[(2 * (ngenes ÷ 3) + 1):ngenes, ((ncells ÷ 2) + 1):ncells] .+= 10
```

Here, you see the simulated data with two groups of cells that differ in the expression of genes.

```julia
heatmap(
    expression_matrix;
    xlabel="cell",
    yflip=true,
    ylabel="gene",
    yticks=1:9,
    colorbar_title="expression",
)
```

While the difference between the two groups of cells here is fairly obvious from looking at the raw data, in practice and with large enough data sets, it is often impossible to spot the differences from the raw data alone.
If you have some patience and compute resources you can increase the size of the dataset, or play around with the noise levels to make the problem increasingly harder.

### pPCA model

```julia
@model function pPCA(; nfeatures::Int, nsamples::Int, nlatent::Int)
    # Latent variables.
    z ~ filldist(Normal(), nlatent, nsamples)

    # Weights/loadings.
    w ~ filldist(Normal(), nfeatures, nlatent)

    # Mean offset.
    μ ~ MvNormal(Zeros(nfeatures), I)

    # Noise variance.
    σ² ~ InverseGamma(1, 1)
    # Data.
    m = w * z .+ μ
    x ~ arraydist(
        map(eachcol(m)) do mi
            return MvNormal(mi, σ² * I)
        end,
    )

    return nothing
end;
```

### pPCA inference

Here, we run the inference with the NUTS sampler.
Feel free to try [different samplers](https://turing.ml/stable/docs/library/#samplers).

```julia
# Create PPCA model with 2 latent dimensions, conditioned on our data.
nlatent = 2
ppca = pPCA(; nfeatures=ngenes, nsamples=ncells, nlatent) | (; x=expression_matrix)

# Sample Markov chain.
chain = sample(ppca, NUTS(), 500);
```

### pPCA control

A quick sanity check.
We reconstruct the input data from our parameter estimates, using the posterior mean as parameter estimates.

```julia
w = [mean(chain, "w[$i,$j]") for i in 1:ngenes, j in 1:nlatent]
z = [mean(chain, "z[$i,$j]") for i in 1:nlatent, j in 1:ncells]
μ = [mean(chain, "μ[$i]") for i in 1:ngenes]
X = w * z .+ μ

heatmap(
    X; xlabel="cell", yflip=true, ylabel="gene", yticks=1:9, colorbar_title="expression"
)
```

We can see the same pattern that we saw in the input data - even though the latent space is only two-dimensional.
The observation is expected if we keep all dimension (`nlatent=ngenes`) since in that case PCA is essentially a lossless transformation, i.e. the new space contains the same information as the input space.

Finally, we plot the data in the lower dimensional latent space.
The interesting insight here is that we can project the information from the input space into a two-dimensional representation, without losing the essential information about the two groups of cells in the input data.

```julia; echo=false
let
    diff = X - expression_matrix
    @assert mean(diff[4, :]) < 0.5
    @assert mean(diff[5, :]) < 0.5
    @assert mean(diff[6, :]) < 0.5
end
```

```julia
heatmap(
    z; xlabel="cell", yflip=true, ylabel="latent dimension", yticks=1:9, colorbar_title="z"
)
```

```julia
scatter(
    z[1, :],
    z[2, :];
    group=repeat(
        ["cells 1-$(ncells ÷ 2)", "cells $((ncells ÷ 2) + 1)-$ncells"]; inner=ncells ÷ 2
    ),
    xlabel="latent dimension \$z_1\$",
    ylabel="latent dimension \$z_2\$",
)
```

We can see the two groups are well separated in this 2-D space.
Another way to put it, 2 dimensions is enough to display the main structure of the data.
Actually, the plots indicate that even a single latent dimension might be sufficient.

## Number of components

A direct question arises from this is:
How many dimensions do we want to keep in order to represent the latent structure in the data?
This is a very central question for all latent factor models, i.e. how many dimensions are needed to represent that data in the latent space.
In the case of PCA, there exist a lot of heuristics to make that choice.
By using the pPCA model, this can be accomplished very elegantly, with a technique called *Automatic Relevance Determination* (ARD).
Essentially, we are using a specific prior over the factor loadings that allows us to prune away dimensions in the latent space.
The prior is determined by a variance hyperparameter $v$, where larger values of $v$ correspond to more important components.
You can find more details about this in Bishop's book (there a precision hyperparameter $\alpha = v^{-1}$ is used instead).

```julia
@model function pPCA_ARD(; nfeatures::Int, nsamples::Int)
    # Latent variables.
    z ~ filldist(Normal(), nfeatures - 1, nsamples)

    # Weights/loadings with automatic relevance determination.
    v ~ filldist(InverseGamma(1.0, 1.0), nfeatures - 1)
    w ~ arraydist(
        map(v) do vi
            return MvNormal(Zeros(nfeatures), vi * I)
        end,
    )

    # Mean offset.
    μ ~ MvNormal(Zeros(nfeatures), I)
    # Noise variance.
    σ² ~ InverseGamma(1, 1)

    # Data.
    m = w * z .+ μ
    x ~ arraydist(
        map(eachcol(m)) do mi
            return MvNormal(mi, σ² * I)
        end,
    )

    return nothing
end

# Create pPCA model, conditioned on our data.
ppca = pPCA_ARD(; nfeatures=ngenes, nsamples=ncells) | (; x=expression_matrix)

# Sample Markov chain.
chain = sample(ppca, NUTS(), 500);
```

We inspect the samples for parameter $v$:

```julia
plot(group(chain, :v))
```

This parameter determines the relevance of individual components.
We can see that they seem to have converged.
Additionally, in one case the samples are centered around much larger values.

We will use the mean of the samples of $v$ to select the two *most relevant* dimensions:
The most relevant latent dimensions are

```julia
v = [mean(chain, "v[$i]") for i in 1:(ngenes - 1)]
relevant = partialsortperm(v, 1:2; rev=true)
```

Based on these two dimensions, we obtain the following reconstruction:

```julia
w = [mean(chain, "w[$i,$j]") for i in 1:ngenes, j in 1:(ngenes - 1)]
z = [mean(chain, "z[$i,$j]") for i in 1:(ngenes - 1), j in 1:ncells]
μ = [mean(chain, "μ[$i]") for i in 1:ngenes]
X = w[:, relevant] * z[relevant, :] .+ μ

heatmap(
    X; xlabel="cell", yflip=true, ylabel="gene", yticks=1:9, colorbar_title="expression"
)
```

As the samples of $v$ indicated, here actually a single latent dimension is sufficient to separate the two types of cells:

```julia
scatter(
    z[relevant[1], :],
    z[relevant[2], :];
    group=repeat(
        ["cells 1-$(ncells ÷ 2)", "cells $((ncells ÷ 2) + 1)-$ncells"]; inner=ncells ÷ 2
    ),
    xlabel="latent dimension \$z_{$(relevant[1])}\$",
    ylabel="latent dimension \$z_{$(relevant[2])}\$",
)
```

Overall, the average latent variables are:

```julia
heatmap(
    z;
    xlabel="cell",
    yflip=true,
    ylabel="latent dimension",
    yticks=1:(ngenes - 1),
    colorbar_title="z",
)
```

These plots are very similar to the lower-dimensional plots above, but here we choose the *relevant* dimensions based on the values of $v$.
When you are in doubt about the number of dimensions to project onto, ARD might provide an answer to that question.

## Batch effects

A second, common aspect apart from the dimensionality of the PCA space is the issue of confounding factors or [batch effects](https://en.wikipedia.org/wiki/Batch_effect).
A batch effect occurs when non-biological factors in an experiment cause changes in the data produced by the experiment.
As an example, we look at Fisher's famous Iris data set.

The data set consists of 50 samples each from three species of Iris (Iris setosa, Iris virginica and Iris versicolor).
Four features were measured from each sample: the length and the width of the sepals and petals (in centimeters).
The Iris dataset is contained in e.g. [RDatasets.jl](https://github.com/JuliaStats/RDatasets.jl).

An example for a batch effect in this case might be different scientists using a different measurement method to determine the length and width of the flowers.
This can lead to a systematic bias in the measurement unrelated to the actual experimental variable - the species in this case.

We load the Iris dataset.

```julia
iris = dataset("datasets", "iris")

# Number of measurements.
nsamples = nrow(iris)

# Measured quantities.
data = iris[!, Not(:Species)]
nfeatures = ncol(data)

# Target variable.
species = iris[!, :Species];
```

First, we look at the original data using the pPCA model with two latent dimensions.

```julia
nlatent = 2
ppca = pPCA(; nfeatures, nsamples, nlatent) | (; x=Matrix(data)')
chain = sample(ppca, NUTS(), 50)
```

```julia
w = [mean(chain, "w[$i,$j]") for i in 1:nfeatures, j in 1:nlatent]
z = [mean(chain, "z[$i,$j]") for i in 1:nlatent, j in 1:nsamples]
μ = [mean(chain, "μ[$i]") for i in 1:nfeatures]
X = w * z .+ μ

heatmap(
    X;
    xlabel="measurement",
    yflip=true,
    ylabel="feature",
    yticks=1:nfeatures,
    yformatter=i -> names(data)[Int(i)],
    colorbar_title="value",
)
```

```julia
heatmap(
    z;
    xlabel="measurement",
    yflip=true,
    ylabel="latent dimension",
    yticks=1:nlatent,
    colorbar_title="z",
)
```

```julia
scatter(
    z[1, :],
    z[2, :];
    group=species,
    xlabel="latent dimension \$z_1\$",
    ylabel="latent dimension \$z_2\$",
)
```

We can see that the setosa species is more clearly separated from the other two species, which overlap considerably.

We now simulate a batch effect:
We assume the person taking the measurement uses two different rulers and they are slightly off.

```julia
batch = rand(Bernoulli(0.5), nsamples)
effect = rand(Normal(2.4, 0.6), nsamples)
batchdata = data .+ batch .* effect;
```

Again, in practice there are many different reasons for why batch effects occur and it is not always clear what is really at the basis of them, nor can they always be tackled via the experimental setup.
So we need methods to deal with them.

```julia
ppca = pPCA(; nfeatures, nsamples, nlatent) | (; x=Matrix(batchdata)')
chain = sample(ppca, NUTS(), 1_000)
```

```julia
z = [mean(chain, "z[$i,$j]") for i in 1:nlatent, j in 1:nsamples]

scatter(
    z[1, :],
    z[2, :];
    group=string.(species, (" (",), batch .+ 1, (")",)),
    xlabel="latent dimension \$z_1\$",
    ylabel="latent dimension \$z_2\$",
)
```

The batch effect makes it much harder to distinguish the species.
And importantly, if we are not aware of the batches, this might lead us to make wrong conclusions about the data.

In order to correct for the batch effect, we need to know about the assignment of measurements to batches.
In our example, this means knowing which ruler was used for which measurement, here encoded via the batch variable.

```julia
@model function pPCA_residual(batch::AbstractVector{Bool}; nfeatures::Int, nlatent::Int)
    # Number of samples
    nsamples = length(batch)

    # Latent variables.
    z ~ filldist(Normal(), nlatent, nsamples)

    # Weights/loadings.
    w ~ filldist(Normal(), nfeatures, nlatent)

    # Covariate vector.
    c ~ MvNormal(Zeros(nfeatures), I)

    # Mean offset.
    μ ~ MvNormal(Zeros(nfeatures), I)

    # Noise variance.
    σ² ~ InverseGamma(1, 1)

    # Data.
    m = w * z + c .* batch' .+ μ
    x ~ arraydist(
        map(eachcol(m)) do mi
            return MvNormal(mi, σ² * I)
        end,
    )

    return nothing
end

ppca = pPCA_residual(batch; nfeatures, nlatent) | (; x=Matrix(batchdata)')

chain = sample(ppca, NUTS(), 1_000)
```

This model is described in considerably more detail [here](https://arxiv.org/abs/1106.4333).

```julia
z = [mean(chain, "z[$i,$j]") for i in 1:nlatent, j in 1:nsamples]

scatter(
    z[1, :],
    z[2, :];
    group=string.(species, (" (",), batch .+ 1, (")",)),
    xlabel="latent dimension \$z_1\$",
    ylabel="latent dimension \$z_2\$",
)
```

We can see that the data are better separated in the latent space by accounting for the batch effect.
It is not perfect but definitely an improvement over the previous approach.

## Rotation Invariant Parameterization

While PCA is an old technique, it is still object of current research.
Here, we implement a [recent publication](http://proceedings.mlr.press/v97/nirwan19a.html) that speeds up inference by removing the rotational symmetry from the posterior.

First we define some helper functions for working with Householder transforms.

```julia
# Normalize the first `ncols` columns of a matrix `V`.
function normalizecols!(V::AbstractMatrix{<:Real}, ncols::Int)
    n = 0
    for v in eachcol(V)
        (n += 1) <= ncols || break
        normalize!(v)
    end
    return V
end

function Householder(k::Int, V)
    v = V[:, k]
    sgn = sign(v[k])

    v[k] += sgn
    H = I - (2.0 / dot(v, v) * (v * v'))
    H[k:end, k:end] = -1.0 * sgn .* H[k:end, k:end]

    return H
end

function H_prod_right(V)
    D, Q = size(V)

    H_prod = zeros(Real, D, D, Q + 1)
    H_prod[:, :, 1] = Diagonal(repeat([1.0], D))

    for q in 1:Q
        H_prod[:, :, q + 1] = Householder(Q - q + 1, V) * H_prod[:, :, q]
    end

    return H_prod
end

function orthogonal_matrix(Q::Int, V)
    normalizecols!(V, Q)
    H_prod = H_prod_right(V)
    return H_prod[:, 1:Q, Q + 1]
end
```

```julia
@model function pPCA_householder(x, K::Int, ::Type{T}=Float64) where {T}
    # Dimensionality of the problem.
    D, N = size(x)
    @assert K <= D

    # parameters
    σ ~ LogNormal(0, 0.5)
    v ~ filldist(Normal(), (K * (D - K + 1)) ÷ 2)
    sigma ~ Bijectors.ordered(MvLogNormal(MvNormal(ones(K))))

    v_mat = zeros(T, D, K)
    v_mat[tril!(trues(size(v_mat)))] .= v
    U = orthogonal_matrix(Q, v_mat)

    W = zeros(T, D, K)
    W += U * Diagonal(sigma)

    Kmat = zeros(T, D, D)
    Kmat += W * W'
    for d in 1:D
        Kmat[d, d] = Kmat[d, d] + σ^2 + 1e-12
    end
    L = cholesky(Kmat).L

    for q in 1:Q
        r2 = sum(abs2, view(v_mat, :, q))
        Turing.@addlogprob! (-log(r2) * (D - q)) / 2
    end

    Turing.@addlogprob! -sum(abs2, sigma) / 2 + (D - Q - 1) * sum(log, sigma)
    for qi in Q:(-1):2
        Turing.@addlogprob! 2 * (qi - 1) * log(sigma[qi]) -
            sum(abs2, view(sigma, 1:(qi - 1)))
    end
    Turing.@addlogprob! logtwo * (K * sum(log, sigma))

    return x ~ filldist(MvNormal(L_full), N)
end;

# Dimensionality of latent space
Random.seed!(1789);
Q = 2
ppca = pPCA_householder(Matrix(dat)', Q)
chain = sample(ppca, NUTS(), 700);

# Extract mean of v from chain
N, D = size(dat)
vv = mean(group(chain, :v))[:, 2]
v_mat = zeros(Real, D, Q)
v_mat[tril!(trues(size(v_mat)))] .= vv
sigma = mean(group(chain, :sigma))[:, 2]
U_n = orthogonal_matrix(Q, v_mat)
W_n = U_n * (I(Q) .* sigma)

# Create array with projected values
z = W_n' * transpose(Matrix(dat))
df_post = DataFrame(convert(Array{Float64}, z)', :auto)
rename!(df_post, Symbol.(["z" * string(i) for i in collect(1:Q)]))
df_post[!, :sample] = 1:n
df_post[!, :species] = species

@vlplot(:point, x = :z1, y = :z2, color = "species:n")(df_post)
```

We observe a speedup in inference, and similar results to the previous implementations. Finally, we will look at the uncertainity that is associated with the samples. We will do this by sampling from the posterior projections. This is possible in the case of the rotation invariant version. If you are curious, you can try to plot the same thing in the case of classical pPCA.

```julia
## Create data projections for each step of chain
vv = collect(get(chain, [:v]).v)
v_mat = zeros(Real, D, Q)
vv_mat = zeros(Float64, n_samples, D, Q)
for i in 1:n_samples
    index = BitArray(zeros(n_samples, D, Q))
    index[i, :, :] = tril!(trues(size(v_mat)))
    tmp = zeros(size(vv)[1])
    for j in 1:(size(vv)[1])
        tmp[j] = vv[j][i]
    end
    vv_mat[index] .= tmp
end

ss = collect(get(chain, [:sigma]).sigma)
sigma = zeros(Q, n_samples)
for d in 1:Q
    sigma[d, :] = Array(ss[d])
end

samples_raw = Array{Float64}(undef, Q, N, n_samples)
for i in 1:n_samples
    U_ni = orthogonal_matrix(D, Q, vv_mat[i, :, :])
    W_ni = U_ni * (LinearAlgebra.I(Q) .* sigma[:, i])
    z_n = W_ni' * transpose(Matrix(dat))
    samples_raw[:, :, i] = z_n
end

# initialize a 3D plot with 1 empty series
plt = plot(
    [100, 200, 300];
    xlim=(-4, 7),
    ylim=(-100, 0),
    group=["Setosa", "Versicolor", "Virginica"],
    markercolor=["red", "blue", "black"],
    title="Visualization",
    seriestype=:scatter,
)

anim = @animate for i in 1:n_samples
    scatter!(
        plt,
        samples_raw[1, 1:50, i],
        samples_raw[2, 1:50, i];
        color="red",
        seriesalpha=0.1,
        label="",
    )
    scatter!(
        plt,
        samples_raw[1, 51:100, i],
        samples_raw[2, 51:100, i];
        color="blue",
        seriesalpha=0.1,
        label="",
    )
    scatter!(
        plt,
        samples_raw[1, 101:150, i],
        samples_raw[2, 101:150, i];
        color="black",
        seriesalpha=0.1,
        label="",
    )
end
gif(anim, "anim_fps.gif"; fps=5)
```

Here we see the density of the projections from the chain to illustrate the uncertainty in the
projections. We can see quite clearly that it is possible to separate Setosa from Versicolor and
Virginica species, but that the latter two cannot be clearly separated based on the pPCA projection. This can be
shown even more clearly by using a kernel density estimate and plotting the contours of that estimate.

```julia; echo=false
let
    m1 = mean(samples_raw[1, 1:50, :])
    m2 = mean(samples_raw[1, 51:100, :])
    m3 = mean(samples_raw[1, 101:150, :])
    @assert m1 - m2 > 3
    @assert m1 - m3 > 3
    @assert m2 - m3 < 2
end
```

```julia
# kernel density estimate
using KernelDensity
dens = kde((vec(samples_raw[1, :, :]), vec(samples_raw[2, :, :])))
plot(dens)
```

```julia, echo=false, skip="notebook", tangle=false
if isdefined(Main, :TuringTutorials)
    Main.TuringTutorials.tutorial_footer(WEAVE_ARGS[:folder], WEAVE_ARGS[:file])
end
```
