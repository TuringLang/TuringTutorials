---
title: Using External Sampler
permalink: /tutorials/:name/
mathjax: true
weave_options:
  error : false
---

# Using External Samplers

`Turing` provides several wrapped samplers from external sampling libraries, e.g. HMC smaplers from `AdvancedHMC`.
These wrappers allow new users to seemlessly sample statistical models without leaving `Turing`
However, these wrappers might not always be complete, missing some functionality from the wrapped sampling library.
Moreover, users might want to use samplers currently not wrapped within `Turing`.

For these reasons, `Turing` also makes it easy to run external samplers on Turing models without any modifications or wrapping being necessary!
Throughout we will use a 10 dimensional Neal's funnel as a running example::

```julia
# Import libraries.
using Turing, Random, LinearAlgebra

d = 10
@model function funnel()
    θ ~ Truncated(Normal(0, 3), -3, 3)
    z ~ MvNormal(zeros(d - 1), exp(θ) * I)
    return x ~ MvNormal(z, I)
end
```

Now we sample the model to generate some observations, which we can then subsequently condition on.

```julia
(; x) = rand(funnel() | (θ=0,))
model = funnel() | (; x)
```

Users can use any sampler algorithm they wish to sample this model as long as it follows the `AbstractMCMC` API.
Before discussing how is this in done in practice, it is interesting to give a high level description of the process.
Imagine that we created an instance of external sampler that we will call `spl` such that `typeof(spl)<:AbstractMCMC.AbstractSampler`.
In order to avoid type ambiguity within Turing, at the moment is necessary to declare `spl` as an external sampler to Turing `espl = externalsampler(spl)` where `externalsampler(s::AbstractMCMC.AbstractSampler)` is a Turing function that types our external sampler adquately.

A good point to start to show how this is done in practice is by looking at the sampling library `AdvancedMH` ((`AdvancedMH`'s GitHub)[[https://github.com/TuringLang/AdvancedMH.jl]) for Metropolis Hastings (MH) methods.
Let's say that we want to use a random walk Metropolis Hastings sampler without specifying the proposal distributions.
The code below constructs a MH sampler using a multivariate Gaussian distribution with zero mean and unit variance in `d` dimensions as a random walk proposal.

```julia
# Importing the sampling library
using AdvancedMH
rwmh = AdvancedMH.RWMH(d)
```

Sampling is then as easy as:

```julia
chain = sample(model, externalsampler(rwmh), 10_000)
```

This process will use `AbstractMCMC` functions overloaded by Turing to generate chains identical to those generated by the native samplers.
Note that if the external sampler produces transitions that Turing cannot parse the bundling of the samples will be different or fail.

## Going beyond the `Turing` API

As previously mentioned the Turing wrappers can often limit the capabilities of the sampling libraries they wrap.
`AdvancedHMC`[^1] ((`AdvancedHMC`'s GitHub)[https://github.com/TuringLang/AdvancedHMC.jl]) is a clear example of this. A common practice when performing HMC is to provide an initial guess for the mass matrix.
However, the native HMC sampler within Turing only allows the user to specify the type of the mass matrix despite the two options being possible within `AdvancedHMC`.
Thankfully, we can use Turing's support for external samplers to define a HMC sampler with a custom mass matrix in `AdvancedHMC` and then use it to sample our Turing model.

We will use the library `Pathfinder`[^2] ((`Pathfinder`'s GitHub)[https://github.com/mlcolab/Pathfinder.jl]) to construct our estimate of mass matrix.
`Pathfinder` is a variational inference algorithm which first finds the maximum a posteriori (MAP) estimate of a target posterior distribution and then uses the trace of the optimization to construct a sequence of multivariate normal approximations to the target distribution.
In this process, `Pathfinder` computes an estimate of the mass matrix that the user can access.

The code below shows can this be done in practice.

```julia
using AdvancedHMC, Pathfidner
# Running pathfidner
draws = 1_000
result_multi = multipathfinder(model, draws; nruns=8)

# Estimating the metric
inv_metric = result_multi.pathfinder_results[1].fit_distribution.Σ
metric = Pathfinder.RankUpdateEuclideanMetric(inv_metric)

# Creating an AdvancedHMC NUTS sampler with the custom metric.
n_adapts = 1000 # Number of adaptation steps
tap = 0.9 # Large target acceptance probability to deal with the funel structure of the posterior
nuts = AdvancedHMC.NUTS(n_adapts, tap; metric=metric)

# Sample
chain = sample(model, externalsampler(nuts), 10_000)
```

## Using new inference methods

So far we have used to Turing's support for external samplers to go beyond the capabilities of the wrappers.
Now we want to use this support to employ a sampler not supported within Turing's ecosystem yet.
We will use the recently developped Micro-Cannoncial Hamiltonian Monte Carlo (MCHMC) sampler to showcase this.
MCHMC[^3,^4] ((MCHMC's GitHub)[https://github.com/JaimeRZP/MicroCanonicalHMC.jl]) is HMC sampler that conserves energy along the exploration of the phase space.
This is achieved by simulating the dynamics of a microcanonical Hamiltonian with an additional noise term to ensure ergodicity.

Using this as well as other inference methods outside the Turing ecosystem is as simple as executing the code shown below:

```julia
using MicroCanonicalHMC
# Create MCHMC sampler
n_adapts = 1_000 # adaptation steps
tev = 0.01 # target energy variance
mchmc = MCHMC(n_adapts, tev; adaptive=true)

# Sample
chain = sample(model, externalsampler(mchmc), 10_000)
```

## The `AbstractMCMC` API

As previously stated, in order to use external sampling libraries within `Turing` they must follow the `AbstractMCMC` API.
In this section we will briefly dwell in what this entails. 
First and foremost, the inference method must be of the type `typeof(spl)<:AbstractMCMC.AbstractSampler`.
Second, the stepping function of the MCMC algorithm must be made into instances of `AbstractMCMC.step` and follow the structure below:

```
# First step
function AbstractMCMC.step{T<:AbstractMCMC.AbstractSampler}(
    rng::Random.AbstractRNG,
    model::AbstractMCMC.LogDensityModel,
    spl::T;
    kwargs...,
)
    [...]
    return transition, sample
end

# N+1 step
function AbstractMCMC.step{T<:AbstractMCMC.AbstractSampler}(
    rng::Random.AbstractRNG,
    model::AbstractMCMC.LogDensityModel,
    sampler::T,
    state;
    kwargs...,
) 
    [...]
    return transition, sample
end
```
There are several characteristic to note in these functions:
- There must be two `step` functions:
  - A function that performs the first step which and initializes the sampler.
  - A function that performs the following steps and which takes and extra input, `state`, which carries the initialization information. 
- The functions must follow the displayed signatures.
- The output of the functions must be a transition, the current state of the sampler, and a sample, what is saved to the MCMC chain.

The last requirement is that the transition must be structure with a field named `θ` which constains the values of the parameters of the model for said transition. 
This allows `Turing` to seemlessly extract the parameter values at each step of the chain when bundling the chains.

For a practical example of how to adapt a sampling library to the `AbstractMCMC` interface the readers can consult the following (code)[https://github.com/JaimeRZP/MicroCanonicalHMC.jl/blob/master/src/abstractmcmc.jl] within `MicroCanonicalHMC`.


# Refences

[^1]: Xu et al, (AdvancedHMC.jl: A robust, modular and efficient implementation of advanced HMC algorithms)[http://proceedings.mlr.press/v118/xu20a/xu20a.pdf], 2019
[^2]: Zhang et al, (Pathfinder: Parallel quasi-Newton variational inference)[https://arxiv.org/abs/2108.03782], 2021
[^3]: Robnik et al, (Microcanonical Hamiltonian Monte Carlo)[https://arxiv.org/abs/2212.08549], 2022
[^4]: Robnik and Seljak, (Langevine Hamiltonian Monte Carlo)[https://arxiv.org/abs/2303.18221], 2023
